{
  "Mistral 7B Instruct v0.3 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\Mistral-7B-Instruct-v0-3-Q6_K-GGUF\Mistral-7B-Instruct-v0.3.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "Mistral 7b uncensored Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\mistral-7b-uncensored-Q6_K-GGUF\mistral-7b-uncensored.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "UTENA 7B NSFW V2 i1 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\UTENA-7B-NSFW-V2-i1-Q6_K-GGUF\UTENA-7B-NSFW-V2.i1-Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "UTENA 7B NSFW Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\UTENA-7B-NSFW-V2-Q6_K-GGUF\UTENA-7B-NSFW-V2.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "dolphin 2.1 mistral 7b Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\dolphin-2.1-mistral-7b-q6_k.GGUF\dolphin-2.1-mistral-7b.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "dolphin 2.9.3 mistral 7B 32k Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\dolphin-2.9.3-mistral-7B-32k-Q6_K-GGUF\dolphin-2.9.3-mistral-7B-32k.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "dragon mistral 0.3 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\dragon-mistral-0.3-Q6_K-GGUF\dragon-mistral-0.3.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "Mistral 7B v0.3 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\Mistral-7B-v0.3-Q6_K-GGUF\Mistral-7B-v0.3.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "MistralLite Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\MistralLite-Q6_K-GGUF\MistralLite.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "mistral orpo capybara 7k Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\mistral-orpo-capybara-7k-Q6_K-GGUF\mistral-orpo-capybara-7k.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 33"
  },
  "OLMo 2 1124 7B Instruct Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\bartowski\OLMo-2-1124-7B-Instruct-Q6_K-GGUF\OLMo-2-1124-7B-Instruct-Q6_K.gguf --slots 4 --metrics --props --port 1234 --gpu-layers 33"
  },
  "OpenAI GPT OSS 20B MXFP4": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\bartowski\OpenAI_GPT-OSS-20B-MXFP4-GGUF\openai_gpt-oss-20b-MXFP4.gguf --slots 4 --metrics --props --port 1234 --ctx-size 8192 --gpu-layers 33 --flash-attn"
  },
  "OpenAI 20B NEO HRRPlus Uncensored IQ4_NL": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\DavidAU\OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL-GGUF\OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf --slots 4 --metrics --props --port 1234 --ctx-size 8192 --gpu-layers 33 --flash-attn"
  },
  "Qwen2.5 7B nerd uncensored v1.0 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\Qwen2.5-7B-nerd-uncensored-v1.0-Q6_K-GGUF\Qwen2.5-7B-nerd-uncensored-v1.0.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 29 --flash-attn"
  },
  "Josiefied Qwen3 8B abliterated v1 Q6_K": {
    "path": "W:/Tools/llama.cpp/build/bin/Release/llama-server.exe",
    "args": "--model W:\AI\LLM\models\mradermacher\Josiefied-Qwen3-8B-abliterated-v1-Q6_K-GGUF\Josiefied-Qwen3-8B-abliterated-v1.Q6_K.gguf --slots 4 --metrics --props --port 1234 --ctx-size 32768 --gpu-layers 37 --flash-attn"
  }
}
